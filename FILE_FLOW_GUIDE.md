# Complete File Flow Guide

**Understanding what each file does and how they interact**

---

## ğŸ¯ Overview - The Complete Flow

```
User Question
      â†“
   main.py (Entry Point)
      â†“
   rag/pipeline.py (Orchestrates everything)
      â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  1. RETRIEVAL (Find relevant docs)  â”‚
   â”‚     rag/hybrid_search.py            â”‚
   â”‚     â†“                               â”‚
   â”‚     ChromaDB (chroma_db/)           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  2. GENERATION (Create answer)      â”‚
   â”‚     llm/assistant.py                â”‚
   â”‚     â†“                               â”‚
   â”‚     Ollama (qwen2.5:7b-instruct)    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  3. METRICS (Track performance)     â”‚
   â”‚     rag/metrics.py                  â”‚
   â”‚     â†“                               â”‚
   â”‚     rag_metrics.jsonl               â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
   Answer with sources + metrics
```

---

## ğŸ“ Core Files - Detailed Flow

### **1. `config.py` - Central Configuration**

**Purpose:** All settings in one place

**What's Inside:**
```python
# LLM Settings
LLM_MODEL_NAME = "qwen2.5:7b-instruct"
LLM_TEMPERATURE = 0.5    # Creativity (0=factual, 1=creative)
LLM_MAX_TOKENS = 256     # Response length

# Embedding Settings
EMBEDDING_MODEL = "BAAI/bge-large-en-v1.5"  # 1024 dimensions

# RAG Settings
RAG_NUM_RESULTS = 5          # How many chunks to retrieve
RAG_MIN_SIMILARITY = 0.30    # Minimum confidence threshold

# System Prompt
GST_SYSTEM_PROMPT = """You are a GST compliance assistant..."""
```

**Flow:**
- **Who uses it:** Every file imports from here
- **Why:** Change settings once, affects entire system
- **Key for:** Model switching, prompt tuning, retrieval tuning

---

### **2. `main.py` - Entry Point**

**Purpose:** User interaction layer

**Flow:**
```
User runs: python main.py "What is ITC?"
    â†“
main.py:
    1. Initialize RAGPipeline
    2. Pass question to pipeline.answer()
    3. Get result (answer + sources + metrics)
    4. Format and display nicely
    5. Save metrics to rag_metrics.jsonl
```

**What It Does:**
- Handles command-line arguments
- Provides interactive chat mode
- Formats output for readability
- Error handling for user

**Key Functions:**
```python
def main():
    pipeline = RAGPipeline()  # Initialize system
    
    if len(sys.argv) > 1:
        # Single question mode
        question = " ".join(sys.argv[1:])
        result = pipeline.answer(question)
        print_result(result)
    else:
        # Interactive chat mode
        pipeline.chat()
```

**Inputs:** User question (string)
**Outputs:** Formatted answer, sources, metrics
**Calls:** `rag/pipeline.py`

---

### **3. `rag/pipeline.py` - RAG Orchestrator**

**Purpose:** The brain of the system - coordinates retrieval + generation

**Complete Flow:**
```
Question: "What is Input Tax Credit?"
    â†“
[STEP 1: INITIALIZATION]
RAGPipeline.__init__():
    1. Load config settings
    2. Connect to ChromaDB (chroma_db/)
    3. Load embedding function (bge-large)
    4. Initialize HybridSearcher
    5. Initialize LLMAssistant
    6. Initialize RAGMetrics
    â†“
[STEP 2: RETRIEVAL]
answer() method:
    1. Start metrics tracking
    2. Call hybrid_search.hybrid_search()
       â†’ Semantic search (ChromaDB)
       â†’ Keyword search (BM25)
       â†’ Combine scores
       â†’ Boost exact term matches
    3. Get top 5 chunks
    4. Filter by min_similarity (0.30)
    5. Log retrieval metrics
    â†“
Retrieved chunks: [
    {text: "ITC means credit of input tax...", 
     source: "cgst-act.pdf", page: 42, similarity: 0.65},
    ...
]
    â†“
[STEP 3: GENERATION]
    1. Build prompt:
       - System prompt (from config)
       - Context (retrieved chunks)
       - User question
    2. Call llm_assistant.generate_with_context()
    3. Get LLM response
    4. Log generation metrics
    â†“
[STEP 4: METRICS]
    1. Calculate faithfulness (answer grounded in context?)
    2. Calculate relevance (answer addresses question?)
    3. Log final metrics
    4. Save to rag_metrics.jsonl
    â†“
Return: {
    question: "What is Input Tax Credit?",
    answer: "Input Tax Credit is...",
    sources: ["cgst-act.pdf, Page 42"],
    confidence: 0.65,
    faithfulness: 0.88,
    relevance: 0.92,
    time_taken: 2.3
}
```

**Key Methods:**
1. `__init__()` - Setup everything
2. `answer(question)` - Main entry point
3. `_format_sources()` - Format citations
4. `chat()` - Interactive mode

**Inputs:** User question
**Outputs:** Complete result dict
**Calls:** 
- `rag/hybrid_search.py`
- `llm/assistant.py`
- `rag/metrics.py`

---

### **4. `rag/hybrid_search.py` - Smart Retrieval**

**Purpose:** Combines semantic + keyword search for better precision

**Flow:**
```
Query: "What is Section 16 about ITC?"
    â†“
hybrid_search():
    â†“
[STEP 1: SEMANTIC SEARCH]
    1. Convert query to embedding (bge-large)
    2. Search ChromaDB by similarity
    3. Get top 10 results
    â†’ Results: [
        {text: "Section 16: Input Tax Credit...", similarity: 0.70},
        {text: "ITC can be claimed...", similarity: 0.60},
        ...
      ]
    â†“
[STEP 2: KEYWORD SEARCH (BM25)]
    1. Tokenize query: ["section", "16", "itc"]
    2. Run BM25 algorithm on all documents
    3. Get top 10 results
    â†’ Results: [
        {text: "Section 16(2): Conditions...", score: 8.5},
        {text: "Section 16(1): Eligibility...", score: 7.2},
        ...
      ]
    â†“
[STEP 3: EXTRACT IMPORTANT TERMS]
    - Finds: "Section 16", "ITC"
    - These will get boosting
    â†“
[STEP 4: COMBINE & RANK]
    For each unique chunk:
        combined_score = (0.7 Ã— semantic_score) + (0.3 Ã— keyword_score)
        
        If chunk contains "Section 16": boost Ã— 1.2
        If chunk contains "ITC": boost Ã— 1.2
        
        final_score = combined_score Ã— boost_factor
    â†“
    Sort by final_score
    â†“
Return top 5 chunks with highest final_score
```

**Why Hybrid?**
- **Semantic:** Understands "ITC" = "Input Tax Credit"
- **Keyword:** Finds exact "Section 16" references
- **Boost:** Prioritizes chunks with specific terms

**Key Components:**
1. `BM25Okapi` - Keyword search algorithm
2. `ChromaDB.query()` - Semantic search
3. `_extract_important_terms()` - Find section/rule numbers
4. `hybrid_search()` - Main orchestration

**Inputs:** Query string, n_results, semantic_weight
**Outputs:** List of chunks with similarity scores
**Used by:** `rag/pipeline.py`

---

### **5. `llm/assistant.py` - LLM Interface**

**Purpose:** Talk to local Ollama model

**Flow:**
```
LLMAssistant.generate_with_context(
    question="What is ITC?",
    context_chunks=[...],
    system_prompt="You are a GST assistant..."
)
    â†“
[STEP 1: BUILD PROMPT]
_build_prompt():
    Combines:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ System Prompt:                     â”‚
    â”‚ "You are a GST assistant..."       â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Context:                           â”‚
    â”‚ [Source 1: cgst-act.pdf, Page 42]  â”‚
    â”‚ Input Tax Credit means...          â”‚
    â”‚                                    â”‚
    â”‚ [Source 2: cgst-act.pdf, Page 43]  â”‚
    â”‚ Conditions for ITC...              â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ User Question:                     â”‚
    â”‚ What is Input Tax Credit?          â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Answer:                            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
[STEP 2: CALL OLLAMA API]
generate():
    1. POST to http://localhost:11434/api/generate
    2. Payload: {
         model: "qwen2.5:7b-instruct",
         prompt: [full_prompt],
         options: {
           temperature: 0.5,
           top_p: 0.9,
           num_predict: 256
         }
       }
    3. Wait for response (2-20 seconds)
    â†“
[STEP 3: RETURN ANSWER]
    "Input Tax Credit (ITC) is the tax paid on purchases 
     which can be set off against tax payable on sales. 
     To claim ITC, you must possess a valid invoice, 
     have received goods/services, and file returns 
     [Source: CGST Act, Section 16, Page 42]."
```

**Key Methods:**
1. `verify_setup()` - Check Ollama is running
2. `generate()` - Raw LLM call
3. `generate_with_context()` - RAG-specific generation
4. `_build_prompt()` - Construct full prompt

**Why Model-Agnostic?**
- Only need to change `LLM_MODEL_NAME` in `config.py`
- Works with any Ollama model
- No model-specific code

**Inputs:** Prompt (string)
**Outputs:** Generated text (string)
**Calls:** Ollama API (localhost:11434)

---

### **6. `rag/metrics.py` - Performance Tracking**

**Purpose:** Log every query for analysis

**Flow:**
```
RAGMetrics lifecycle for one query:
    â†“
[START]
start_query("What is ITC?"):
    current_query_data = {
        question: "What is ITC?",
        timestamp: "2026-01-01T10:00:00",
        start_time: 1234567890.0,
        ...all fields initialized...
    }
    â†“
[RETRIEVAL]
log_retrieval(
    chunks_retrieved=10,
    chunks_used=5,
    avg_similarity=0.52,
    ...
):
    Update current_query_data:
        chunks_retrieved: 10
        chunks_used: 5
        avg_similarity: 0.52
        retrieval_efficiency: 5/10 = 0.5
    â†“
[GENERATION]
log_generation(
    answer="Input Tax Credit is...",
    generation_time=2.1,
    ...
):
    Update current_query_data:
        answer_length: 250
        answer_words: 45
        generation_time: 2.1
        response_quality_flag: "good"
    â†“
[FINALIZE]
finalize_query(
    total_time=3.5,
    faithfulness=0.88,
    relevance=0.92,
    ...
):
    Update current_query_data:
        total_time: 3.5
        faithfulness: 0.88
        relevance: 0.92
        success: True
        efficiency_score: 0.52 / 3.5 = 0.15
    
    Write to rag_metrics.jsonl:
        {full JSON of all metrics}
    
    Add to query_history (in-memory)
```

**What's Logged:**
```json
{
  "question": "What is Input Tax Credit?",
  "timestamp": "2026-01-01T10:00:00",
  "chunks_retrieved": 10,
  "chunks_used": 5,
  "avg_similarity": 0.52,
  "top_similarity": 0.65,
  "retrieval_time": 0.8,
  "retrieval_efficiency": 0.5,
  "answer_length": 250,
  "answer_words": 45,
  "generation_time": 2.1,
  "total_time": 3.5,
  "confidence_score": 0.52,
  "faithfulness": 0.88,
  "relevance": 0.92,
  "response_quality_flag": "good",
  "efficiency_score": 0.15,
  "success": true,
  "error": null
}
```

**Key Methods:**
1. `start_query()` - Begin tracking
2. `log_retrieval()` - Log retrieval phase
3. `log_generation()` - Log generation phase
4. `finalize_query()` - Write to file
5. `get_summary_statistics()` - Aggregate metrics
6. `calculate_faithfulness()` - Check if grounded in context
7. `calculate_relevance()` - Check if answers question

**Outputs:** `rag_metrics.jsonl` (one JSON per line)
**Used by:** `rag/pipeline.py`, `view_metrics.py`

---

### **7. `rag/enhanced_chunker.py` - Smart Document Splitting**

**Purpose:** Split documents while preserving meaning and adding context

**Flow:**
```
Input: PDF text (294 pages)
    â†“
EnhancedSemanticChunker.chunk(text, metadata):
    â†“
[STEP 1: EXTRACT DOCUMENT CONTEXT]
    - Document title: "CGST Act 2017"
    - Document type: "gst_legal"
    â†“
[STEP 2: FIND SEMANTIC BOUNDARIES]
    Patterns:
    - "Section 16:"  â†’ boundary
    - "Rule 42:"     â†’ boundary
    - "\n\n"         â†’ boundary (paragraph)
    - "(a)"          â†’ boundary (sub-point)
    
    Text split into segments at these points
    â†“
[STEP 3: SENTENCE-AWARE SPLITTING]
    For each segment:
        1. Use NLTK to detect sentence boundaries
        2. Get semantic embeddings for sentences
        3. Calculate similarity between consecutive sentences
        4. If similarity < threshold â†’ new chunk boundary
        5. Merge similar sentences together
    
    Example:
        Original segment:
        "Section 16: Input Tax Credit. ITC means credit. 
         The person shall be entitled. Conditions apply."
        
        After sentence detection:
        ["Section 16: Input Tax Credit.",
         "ITC means credit.",
         "The person shall be entitled.",
         "Conditions apply."]
        
        After semantic merging (if similar):
        ["Section 16: Input Tax Credit. ITC means credit.",
         "The person shall be entitled. Conditions apply."]
    â†“
[STEP 4: SMART SIZING]
    For each potential chunk:
        - If < min_size (200) â†’ merge with next
        - If > max_size (1200) â†’ backtrack to last complete sentence
        - Never break mid-sentence!
    â†“
[STEP 5: CONTEXT ENRICHMENT]
    For each chunk:
        Prepend context:
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Document: CGST Act 2017             â”‚
        â”‚ Section: Section 16                 â”‚
        â”‚ Type: GST Legal Document            â”‚
        â”‚ Page: 42                            â”‚
        â”‚                                     â”‚
        â”‚ [original chunk text]               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    This context gets embedded too!
    â†“
[STEP 6: METADATA ENRICHMENT]
    Add metadata:
        {
            source: "cgst-act.pdf",
            page: 42,
            document_title: "CGST Act 2017",
            section_id: "Section 16",
            section_title: "Input Tax Credit",
            document_type: "gst_legal",
            chunk_index: 0,
            char_start: 1200,
            char_end: 2350,
            chunk_size: 1150,
            chunking_strategy: "enhanced_semantic"
        }
    â†“
Output: List of enriched chunks
    [
        {
            text: "Document: CGST Act 2017\nSection: Section 16\n...",
            metadata: {...}
        },
        ...
    ]
```

**Why Enhanced?**
1. **Semantic Boundaries** - Respects document structure
2. **Sentence-Aware** - Never breaks mid-sentence
3. **Context-Enriched** - Adds document/section context
4. **Metadata-Rich** - Tracks everything

**Key Methods:**
1. `_get_document_title()` - Extract title from first lines
2. `_find_semantic_boundaries()` - Find section/rule/paragraph breaks
3. `_split_into_sentences_semantically()` - Sentence detection + merging
4. `_extract_section_info()` - Extract section ID and title
5. `chunk()` - Main orchestration

**Inputs:** Text + base metadata
**Outputs:** List of enriched chunks
**Used by:** `scripts/ingest_pdfs.py`

---

## ğŸ”§ Script Files - One-Time Operations

### **8. `scripts/ingest_pdfs.py` - Data Pipeline**

**Purpose:** Convert PDFs â†’ ChromaDB

**Complete Flow:**
```
Run: python scripts/ingest_pdfs.py
    â†“
[INITIALIZATION]
GSTProcessor.__init__():
    1. Load embedding model (bge-large, ~1.3GB download)
    2. Connect to ChromaDB (./chroma_db/)
    3. Get or create collection "gst_rules"
    4. Initialize EnhancedSemanticChunker
    â†“
[FIND PDFs]
process_all_pdfs():
    1. Scan data/gst/ folder
    2. Find all .pdf files
    3. List them: cgst-act.pdf (2.1 MB), cgst-rules.pdf (1.8 MB)
    â†“
[PROCESS EACH PDF]
For each PDF:
    â†“
    [STEP 1: EXTRACT]
    extract_text_from_pdf():
        1. Open PDF with pdfplumber
        2. Extract text page by page
        3. Filter empty pages
        â†’ Result: [
            {page: 1, text: "CGST Act 2017..."},
            {page: 2, text: "Section 1..."},
            ...
          ]
    â†“
    [STEP 2: CHUNK]
    For each page:
        chunker.chunk(text, metadata):
            1. Find semantic boundaries
            2. Split sentences intelligently
            3. Size chunks appropriately
            4. Add context enrichment
            5. Attach metadata
    
    â†’ Result: 855 chunks from 294 pages
    â†“
    [STEP 3: PREPARE]
    For each chunk:
        1. Generate unique ID: "cgst-act_0", "cgst-act_1", ...
        2. Extract text
        3. Clean metadata (remove None values)
    
    â†’ Arrays:
        ids = ["cgst-act_0", "cgst-act_1", ...]
        documents = ["Document: CGST...", "Document: CGST...", ...]
        metadatas = [{source: ..., page: ...}, {...}, ...]
    â†“
    [STEP 4: EMBED & STORE]
    collection.add(ids, documents, metadatas):
        ChromaDB automatically:
        1. Creates embeddings using bge-large (1024-dim)
        2. Stores embeddings in ./chroma_db/
        3. Indexes for fast retrieval
        4. Persists to disk
    
    Progress: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 855/855 chunks
    â†“
[COMPLETION]
    Print statistics:
    âœ… 294 pages processed
    âœ… 855 chunks created
    âœ… Database: ./chroma_db/
    âœ… Ready for queries!
```

**When to Run:**
- First time setup
- After adding new PDFs
- After changing chunking strategy
- After corrupting database

**Inputs:** PDF files in `data/gst/`
**Outputs:** ChromaDB database in `chroma_db/`
**Time:** 2-5 minutes (first run with model download: 5-10 min)

---

### **9. `view_metrics.py` - Analytics**

**Purpose:** View performance summary

**Flow:**
```
Run: python view_metrics.py
    â†“
Load rag_metrics.jsonl:
    Read all lines
    Parse each JSON
    â†’ query_history = [query1, query2, ...]
    â†“
Calculate statistics:
    total_queries = 50
    successful = 48
    success_rate = 48/50 = 96%
    
    avg_confidence = sum(confidence) / 50 = 0.36
    avg_faithfulness = sum(faithfulness) / 50 = 0.78
    avg_relevance = sum(relevance) / 50 = 0.82
    
    avg_retrieval_time = 0.8s
    avg_generation_time = 2.1s
    avg_total_time = 3.5s
    
    response_quality:
        good: 40
        too_short: 5
        verbose: 3
        unknown: 2
    â†“
Print formatted summary:
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘  RAG Performance Summary             â•‘
    â•‘  (Last 50 queries)                   â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  Total Queries: 50                   â•‘
    â•‘  Success Rate: 96%                   â•‘
    â•‘  Avg Confidence: 36%                 â•‘
    â•‘  Avg Faithfulness: 78%               â•‘
    â•‘  Avg Relevance: 82%                  â•‘
    â•‘  ...                                 â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Options:**
```bash
python view_metrics.py          # All queries
python view_metrics.py --last 10  # Last 10 only
```

**Inputs:** `rag_metrics.jsonl`
**Outputs:** Console summary

---

## ğŸ§ª Test Files

### **10. `tests/test_questions.json` - Ground Truth**

**Purpose:** 50 test questions with expected answers

**Structure:**
```json
[
  {
    "id": 1,
    "category": "factual",
    "difficulty": "easy",
    "question": "What is Input Tax Credit?",
    "expected_keywords": ["credit", "input tax", "set off"],
    "expected_source": "cgst-act.pdf",
    "min_confidence": 0.4
  },
  {
    "id": 2,
    "category": "procedural",
    "difficulty": "medium",
    "question": "How to claim ITC under Section 16?",
    "expected_keywords": ["invoice", "received", "returns", "filed"],
    "expected_source": "cgst-act.pdf",
    "min_confidence": 0.35
  },
  ...
]
```

**Categories:**
- `factual` - "What is X?"
- `procedural` - "How to do X?"
- `analytical` - "Why/How many?"
- `reference` - "Section X says?"

**Used by:** `tests/evaluate_assistant.py`

---

### **11. `tests/evaluate_assistant.py` - Automated Testing**

**Purpose:** Test assistant against ground truth

**Flow:**
```
Run: python tests/evaluate_assistant.py
    â†“
Load test_questions.json:
    50 questions
    â†“
For each question:
    â†“
    [ASK ASSISTANT]
    result = pipeline.answer(question)
    
    [EVALUATE]
    1. Keyword Match:
       expected = ["credit", "input tax"]
       answer_text = result['answer'].lower()
       
       matches = [kw for kw in expected if kw in answer_text]
       keyword_score = len(matches) / len(expected)
    
    2. Source Match:
       expected_source = "cgst-act.pdf"
       actual_sources = result['sources']
       
       source_match = expected_source in str(actual_sources)
    
    3. Confidence Check:
       min_confidence = 0.4
       actual_confidence = result['confidence']
       
       confidence_ok = actual_confidence >= min_confidence
    
    4. Faithfulness/Relevance:
       faithfulness = result['faithfulness']
       relevance = result['relevance']
    
    [DETERMINE PASS/FAIL]
    If (keyword_score >= 0.5 AND 
        source_match AND 
        confidence_ok):
        âœ… PASS
    else:
        âŒ FAIL
    â†“
[AGGREGATE RESULTS]
    Overall:
        Pass Rate: 32/50 = 64%
    
    By Category:
        Factual: 18/20 = 90%
        Procedural: 10/15 = 67%
        Analytical: 4/15 = 27%
    
    By Difficulty:
        Easy: 15/15 = 100%
        Medium: 12/20 = 60%
        Hard: 5/15 = 33%
    
    Avg Metrics:
        Confidence: 36%
        Faithfulness: 78%
        Relevance: 82%
        Response Time: 3.5s
    â†“
[GENERATE REPORT]
    Save to: evaluation_report_TIMESTAMP.json
    Print summary to console
    Identify weakest categories
    Suggest improvements
```

**Options:**
```bash
python tests/evaluate_assistant.py              # All 50 questions
python tests/evaluate_assistant.py --limit 10   # First 10 only
```

**Inputs:** `tests/test_questions.json`
**Outputs:** 
- Console report
- `evaluation_report_YYYYMMDD_HHMMSS.json`

---

### **12. `tests/verify_documents.py` - Coverage Check**

**Purpose:** Can documents answer test questions?

**Flow:**
```
Run: python tests/verify_documents.py
    â†“
Load test_questions.json
    â†“
For each question:
    â†“
    Extract expected keywords
    Extract expected source
    â†“
    Search ChromaDB directly:
        results = collection.query(
            query_texts=[question],
            n_results=5
        )
    â†“
    Check if keywords appear in results:
        If 80% of keywords found:
            âœ… ANSWERABLE
        else:
            âŒ MISSING DATA
    â†“
Aggregate:
    Answerable: 44/50 = 88%
    Missing: 6/50 = 12%
    
    Missing categories:
    - Analytical questions (need calculation)
    - Recent GST updates (not in documents)
    - State-specific rules (only have CGST)
```

**Purpose:** Validate that your documents CAN answer the questions (before blaming RAG/LLM)

**Inputs:** `tests/test_questions.json`, ChromaDB
**Outputs:** Coverage report

---

### **13. `tests/test_search.py` - Retrieval Tests**

**Purpose:** Test ChromaDB retrieval quality

**Flow:**
```
Run: python tests/test_search.py
    â†“
[TEST 1: Collection Exists]
    Connect to ChromaDB
    Check collection "gst_rules"
    Count documents
    â†’ Expected: >0
    â†“
[TEST 2: Metadata Completeness]
    Sample 10 random documents
    Check each has:
        - source
        - page
        - document_type
        - chunk_index
    â†’ Expected: 100% complete
    â†“
[TEST 3: Semantic Search]
    Test queries:
        - "How to claim Input Tax Credit?"
        - "What is reverse charge mechanism?"
        - "Time limit for filing GSTR-1"
    
    For each:
        results = collection.query(query, n_results=5)
        
        Check:
        1. Got 5 results
        2. Similarity > 0.3
        3. Relevant keywords in results
        4. Sources cited
    â†’ Expected: 80% pass
    â†“
[TEST 4: Chunk Quality]
    Sample 20 chunks
    For each:
        1. Length in range (200-1500 chars)
        2. Ends with complete sentence
        3. Has section_id (if applicable)
        4. Text is readable
    â†’ Expected: 90% pass
    â†“
[TEST 5: Interactive Search]
    Prompt user for custom queries
    Show top results
    Display metadata
```

**When to Run:**
- After ingestion
- After changing chunking
- When debugging retrieval issues

**Inputs:** ChromaDB
**Outputs:** Test results + interactive search

---

## ğŸ”„ Complete Query Flow - Step by Step

Let's trace a complete query: **"What is Input Tax Credit?"**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USER                                                         â”‚
â”‚ $ python main.py "What is Input Tax Credit?"               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ main.py                                                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ 1. Parse arguments                                       â”‚ â”‚
â”‚ â”‚    question = "What is Input Tax Credit?"               â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚ 2. Initialize RAGPipeline                               â”‚ â”‚
â”‚ â”‚    pipeline = RAGPipeline()                             â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ rag/pipeline.py - RAGPipeline.__init__()                    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ 1. Load config from config.py                           â”‚ â”‚
â”‚ â”‚    LLM_MODEL_NAME = "qwen2.5:7b-instruct"              â”‚ â”‚
â”‚ â”‚    EMBEDDING_MODEL = "bge-large-en-v1.5"               â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚ 2. Connect to ChromaDB                                  â”‚ â”‚
â”‚ â”‚    client = chromadb.PersistentClient("./chroma_db")   â”‚ â”‚
â”‚ â”‚    collection = client.get_collection("gst_rules")     â”‚ â”‚
â”‚ â”‚    â†’ 855 documents loaded                               â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚ 3. Initialize HybridSearcher                            â”‚ â”‚
â”‚ â”‚    hybrid_searcher = HybridSearcher(collection)        â”‚ â”‚
â”‚ â”‚    â†’ BM25 index built                                   â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚ 4. Initialize LLMAssistant                              â”‚ â”‚
â”‚ â”‚    llm_assistant = LLMAssistant("qwen2.5:7b")          â”‚ â”‚
â”‚ â”‚    â†’ Verify Ollama running âœ“                            â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚ 5. Initialize RAGMetrics                                â”‚ â”‚
â”‚ â”‚    metrics = RAGMetrics()                               â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ rag/pipeline.py - answer()                                   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ metrics.start_query("What is Input Tax Credit?")       â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ rag/hybrid_search.py - hybrid_search()                       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ [SEMANTIC SEARCH]                                        â”‚ â”‚
â”‚ â”‚ 1. Embed query using bge-large                          â”‚ â”‚
â”‚ â”‚    query_embedding = [0.23, -0.45, 0.67, ...]          â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚ 2. Search ChromaDB                                      â”‚ â”‚
â”‚ â”‚    results = collection.query(embedding, n=10)         â”‚ â”‚
â”‚ â”‚    â†’ Top results:                                        â”‚ â”‚
â”‚ â”‚       [0.70] "Section 16: Input Tax Credit means..."   â”‚ â”‚
â”‚ â”‚       [0.65] "ITC is credit of input tax paid..."      â”‚ â”‚
â”‚ â”‚       [0.58] "Conditions for claiming ITC..."          â”‚ â”‚
â”‚ â”‚       ...                                               â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚ [KEYWORD SEARCH]                                        â”‚ â”‚
â”‚ â”‚ 3. Tokenize query                                       â”‚ â”‚
â”‚ â”‚    tokens = ["input", "tax", "credit"]                 â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚ 4. Run BM25                                             â”‚ â”‚
â”‚ â”‚    bm25_scores = [8.5, 7.2, 6.8, ...]                  â”‚ â”‚
â”‚ â”‚    â†’ Top results:                                        â”‚ â”‚
â”‚ â”‚       [8.5] "Input Tax Credit shall be..."             â”‚ â”‚
â”‚ â”‚       [7.2] "Section 16(2): Conditions for ITC"        â”‚ â”‚
â”‚ â”‚       ...                                               â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚ [COMBINE]                                               â”‚ â”‚
â”‚ â”‚ 5. For each unique chunk:                               â”‚ â”‚
â”‚ â”‚    score = (0.7 Ã— semantic) + (0.3 Ã— bm25)            â”‚ â”‚
â”‚ â”‚    if contains "ITC": score Ã— 1.2                      â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚ 6. Sort and return top 5                                â”‚ â”‚
â”‚ â”‚    â†’ Chunks: [                                          â”‚ â”‚
â”‚ â”‚        {text: "Section 16...", similarity: 0.72},      â”‚ â”‚
â”‚ â”‚        {text: "ITC means...", similarity: 0.68},       â”‚ â”‚
â”‚ â”‚        {text: "Conditions...", similarity: 0.61},      â”‚ â”‚
â”‚ â”‚        {text: "Time limit...", similarity: 0.55},      â”‚ â”‚
â”‚ â”‚        {text: "Documentation...", similarity: 0.48}    â”‚ â”‚
â”‚ â”‚      ]                                                  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“ (returns chunks)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ rag/pipeline.py - answer() [continued]                       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ 7. Filter by min_similarity (0.30)                      â”‚ â”‚
â”‚ â”‚    â†’ All 5 chunks pass                                  â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚ 8. Log retrieval metrics                                â”‚ â”‚
â”‚ â”‚    metrics.log_retrieval(                               â”‚ â”‚
â”‚ â”‚      chunks_retrieved=5,                                â”‚ â”‚
â”‚ â”‚      avg_similarity=0.61,                               â”‚ â”‚
â”‚ â”‚      retrieval_time=0.8s                                â”‚ â”‚
â”‚ â”‚    )                                                     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ llm/assistant.py - generate_with_context()                   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ 9. Build prompt                                          â”‚ â”‚
â”‚ â”‚    full_prompt = """                                    â”‚ â”‚
â”‚ â”‚    You are a GST compliance assistant.                  â”‚ â”‚
â”‚ â”‚    Answer ONLY from context. Be CONCISE.               â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚    Context:                                             â”‚ â”‚
â”‚ â”‚    [Source 1: cgst-act.pdf, Page 42]                   â”‚ â”‚
â”‚ â”‚    Section 16: Input Tax Credit                        â”‚ â”‚
â”‚ â”‚    ITC means the credit of input tax paid on           â”‚ â”‚
â”‚ â”‚    purchases which can be set off against output       â”‚ â”‚
â”‚ â”‚    tax liability.                                       â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚    [Source 2: cgst-act.pdf, Page 43]                   â”‚ â”‚
â”‚ â”‚    Conditions for claiming ITC under Section 16(2):    â”‚ â”‚
â”‚ â”‚    1. Valid tax invoice                                 â”‚ â”‚
â”‚ â”‚    2. Goods/services received                          â”‚ â”‚
â”‚ â”‚    3. Tax paid to government                           â”‚ â”‚
â”‚ â”‚    4. Returns filed                                     â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚    ...3 more sources...                                 â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚    User Question: What is Input Tax Credit?            â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚    Answer:                                              â”‚ â”‚
â”‚ â”‚    """                                                  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ llm/assistant.py - generate()                                â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ 10. Call Ollama API                                      â”‚ â”‚
â”‚ â”‚     POST http://localhost:11434/api/generate            â”‚ â”‚
â”‚ â”‚     {                                                    â”‚ â”‚
â”‚ â”‚       model: "qwen2.5:7b-instruct",                     â”‚ â”‚
â”‚ â”‚       prompt: [full_prompt],                            â”‚ â”‚
â”‚ â”‚       options: {temperature: 0.5, max_tokens: 256}     â”‚ â”‚
â”‚ â”‚     }                                                    â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚ 11. Wait for LLM response (2-20 seconds)                â”‚ â”‚
â”‚ â”‚     â³ Generating...                                     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“ (2.1 seconds later)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama (qwen2.5:7b-instruct)                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ [LLM GENERATES]                                          â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚ "Input Tax Credit (ITC) is the credit of input tax     â”‚ â”‚
â”‚ â”‚  paid on purchases of goods and services, which can    â”‚ â”‚
â”‚ â”‚  be set off against the output tax liability on        â”‚ â”‚
â”‚ â”‚  sales. To claim ITC under Section 16, you must:       â”‚ â”‚
â”‚ â”‚  1. Possess a valid tax invoice                         â”‚ â”‚
â”‚ â”‚  2. Have received the goods or services                 â”‚ â”‚
â”‚ â”‚  3. Ensure tax has been paid to the government         â”‚ â”‚
â”‚ â”‚  4. File your returns on time                          â”‚ â”‚
â”‚ â”‚  [Source: CGST Act, Section 16, Page 42-43]"           â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“ (returns answer)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ rag/pipeline.py - answer() [continued]                       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ 12. Log generation metrics                               â”‚ â”‚
â”‚ â”‚     metrics.log_generation(                              â”‚ â”‚
â”‚ â”‚       answer=llm_answer,                                 â”‚ â”‚
â”‚ â”‚       generation_time=2.1s                               â”‚ â”‚
â”‚ â”‚     )                                                     â”‚ â”‚
â”‚ â”‚                                                           â”‚ â”‚
â”‚ â”‚ 13. Calculate faithfulness & relevance                   â”‚ â”‚
â”‚ â”‚     faithfulness = 0.88  (88% grounded in context)      â”‚ â”‚
â”‚ â”‚     relevance = 0.92     (92% addresses question)       â”‚ â”‚
â”‚ â”‚                                                           â”‚ â”‚
â”‚ â”‚ 14. Finalize metrics                                     â”‚ â”‚
â”‚ â”‚     metrics.finalize_query(                              â”‚ â”‚
â”‚ â”‚       total_time=3.5s,                                   â”‚ â”‚
â”‚ â”‚       faithfulness=0.88,                                 â”‚ â”‚
â”‚ â”‚       relevance=0.92                                     â”‚ â”‚
â”‚ â”‚     )                                                     â”‚ â”‚
â”‚ â”‚     â†’ Saved to rag_metrics.jsonl                         â”‚ â”‚
â”‚ â”‚                                                           â”‚ â”‚
â”‚ â”‚ 15. Return result                                        â”‚ â”‚
â”‚ â”‚     return {                                             â”‚ â”‚
â”‚ â”‚       question: "What is Input Tax Credit?",            â”‚ â”‚
â”‚ â”‚       answer: "Input Tax Credit (ITC) is...",           â”‚ â”‚
â”‚ â”‚       sources: ["cgst-act.pdf, Page 42", ...],          â”‚ â”‚
â”‚ â”‚       confidence: 0.61,                                  â”‚ â”‚
â”‚ â”‚       faithfulness: 0.88,                                â”‚ â”‚
â”‚ â”‚       relevance: 0.92,                                   â”‚ â”‚
â”‚ â”‚       chunks_used: 5,                                    â”‚ â”‚
â”‚ â”‚       time_taken: 3.5                                    â”‚ â”‚
â”‚ â”‚     }                                                     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“ (returns to main.py)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ main.py - print_result()                                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ 16. Format output                                        â”‚ â”‚
â”‚ â”‚     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•        â”‚ â”‚
â”‚ â”‚     Question: What is Input Tax Credit?                 â”‚ â”‚
â”‚ â”‚     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•        â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚     Input Tax Credit (ITC) is the credit of input      â”‚ â”‚
â”‚ â”‚     tax paid on purchases of goods and services,       â”‚ â”‚
â”‚ â”‚     which can be set off against the output tax        â”‚ â”‚
â”‚ â”‚     liability on sales. To claim ITC under             â”‚ â”‚
â”‚ â”‚     Section 16, you must:                              â”‚ â”‚
â”‚ â”‚     1. Possess a valid tax invoice                      â”‚ â”‚
â”‚ â”‚     2. Have received the goods or services             â”‚ â”‚
â”‚ â”‚     3. Ensure tax has been paid to the government      â”‚ â”‚
â”‚ â”‚     4. File your returns on time                       â”‚ â”‚
â”‚ â”‚     [Source: CGST Act, Section 16, Page 42-43]         â”‚ â”‚
â”‚ â”‚                                                          â”‚ â”‚
â”‚ â”‚     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•        â”‚ â”‚
â”‚ â”‚     Sources:                                            â”‚ â”‚
â”‚ â”‚       1. cgst-act.pdf, Page 42 (72% similarity)        â”‚ â”‚
â”‚ â”‚       2. cgst-act.pdf, Page 43 (68% similarity)        â”‚ â”‚
â”‚ â”‚       3. cgst-act.pdf, Page 44 (61% similarity)        â”‚ â”‚
â”‚ â”‚     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•        â”‚ â”‚
â”‚ â”‚     Confidence: 61%                                     â”‚ â”‚
â”‚ â”‚     Faithfulness: 88%                                   â”‚ â”‚
â”‚ â”‚     Relevance: 92%                                      â”‚ â”‚
â”‚ â”‚     Chunks used: 5                                      â”‚ â”‚
â”‚ â”‚     Time taken: 3.5s                                    â”‚ â”‚
â”‚ â”‚     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•        â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USER sees answer âœ…                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Total Flow:**
1. User â†’ `main.py` (parse)
2. `main.py` â†’ `rag/pipeline.py` (initialize)
3. `rag/pipeline.py` â†’ `rag/hybrid_search.py` (retrieve)
4. `rag/hybrid_search.py` â†’ ChromaDB (semantic search)
5. `rag/hybrid_search.py` â†’ BM25 (keyword search)
6. `rag/hybrid_search.py` â†’ `rag/pipeline.py` (return chunks)
7. `rag/pipeline.py` â†’ `llm/assistant.py` (generate)
8. `llm/assistant.py` â†’ Ollama API (LLM call)
9. Ollama â†’ `llm/assistant.py` (answer)
10. `llm/assistant.py` â†’ `rag/pipeline.py` (return)
11. `rag/pipeline.py` â†’ `rag/metrics.py` (log)
12. `rag/pipeline.py` â†’ `main.py` (return result)
13. `main.py` â†’ User (display)

**Time Breakdown:**
- Retrieval: 0.8s (semantic + keyword search)
- Generation: 2.1s (LLM thinking)
- Metrics: 0.1s (logging)
- Formatting: 0.5s (display)
- **Total: 3.5s**

---

## ğŸ“ Key Takeaways

### **File Responsibilities:**

| File | Role | When It Runs |
|------|------|-------------|
| `config.py` | Settings storage | Imported by all |
| `main.py` | User interface | Every query |
| `rag/pipeline.py` | Orchestrator | Every query |
| `rag/hybrid_search.py` | Retrieval | Every query |
| `rag/enhanced_chunker.py` | Document splitting | During ingestion |
| `llm/assistant.py` | LLM interface | Every query |
| `rag/metrics.py` | Performance tracking | Every query |
| `scripts/ingest_pdfs.py` | Data pipeline | One-time / updates |
| `tests/evaluate_assistant.py` | Testing | When validating |
| `view_metrics.py` | Analytics | When analyzing |

### **Data Flow:**
```
PDFs â†’ ingest_pdfs.py â†’ ChromaDB
                            â†“
User Query â†’ pipeline.py â†’ hybrid_search.py â†’ ChromaDB
                            â†“                     â†“
                         chunks             embeddings
                            â†“
                      llm/assistant.py â†’ Ollama
                            â†“
                         answer
                            â†“
                      metrics.py â†’ rag_metrics.jsonl
                            â†“
                          User
```

### **Configuration Flow:**
```
config.py
   â†“
   â”œâ”€â†’ main.py (gets all settings)
   â”œâ”€â†’ rag/pipeline.py (RAG settings)
   â”œâ”€â†’ llm/assistant.py (LLM settings)
   â”œâ”€â†’ rag/hybrid_search.py (retrieval settings)
   â””â”€â†’ scripts/ingest_pdfs.py (embedding settings)
```

---

**Now you understand every file and how they work together!** ğŸ¯

